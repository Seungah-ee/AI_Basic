{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니 배치 학습의 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "x = cancer.data\n",
    "y = cancer.target\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(x, y, stratify=y, \n",
    "                                                            test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, \n",
    "                                                  test_size=0.2, random_state=42)\n",
    "\n",
    "class SingleLayer:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.w = None              # 가중치\n",
    "        self.b = None              # 절편\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.w_history = []        # 가중치 기록\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z = np.dot(x, self.w) + self.b        # 선형 출력을 계산합니다.\n",
    "        return z\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)\n",
    "        w_grad = np.dot(x.T, err) / m         # 가중치에 대한 그래디언트를 계산합니다.\n",
    "        b_grad = np.sum(err) / m              # 절편에 대한 그래디언트를 계산합니다.\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def activation(self, z):\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산\n",
    "        return a\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)                  # 타깃을 열 벡터로 바꿉니다.\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        m = len(x)                            # 샘플 개수를 저장합니다.\n",
    "        self.w = np.ones((x.shape[1], 1))     # 가중치를 초기화합니다.\n",
    "        self.b = 0                            # 절편을 초기화합니다.\n",
    "        self.w_history.append(self.w.copy())  # 가중치를 기록합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            z = self.forpass(x)               # 정방향 계산을 수행합니다.\n",
    "            a = self.activation(z)            # 활성화 함수를 적용합니다.\n",
    "            err = -(y - a)                    # 오차를 계산합니다.\n",
    "            # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "            w_grad, b_grad = self.backprop(x, err)\n",
    "            # 그래디언트에서 페널티 항의 미분 값을 더합니다.\n",
    "            w_grad += (self.l1 * np.sign(self.w) + self.l2 * self.w) / m\n",
    "            # 가중치와 절편을 업데이트합니다.\n",
    "            self.w -= self.lr * w_grad\n",
    "            self.b -= self.lr * b_grad\n",
    "            # 가중치를 기록합니다.\n",
    "            self.w_history.append(self.w.copy())\n",
    "            # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / m)\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)      # 정방향 계산을 수행합니다.\n",
    "        return z > 0             # 스텝 함수를 적용합니다.\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
    "        return np.mean(self.predict(x) == y.reshape(-1, 1))\n",
    "    \n",
    "    def reg_loss(self):\n",
    "        # 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * np.sum(np.abs(self.w)) + self.l2 / 2 * np.sum(self.w**2)\n",
    "    \n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)            # 정방향 계산을 수행합니다.\n",
    "        a = self.activation(z)             # 활성화 함수를 적용합니다.\n",
    "        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값을 클리핑합니다.\n",
    "        # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "        val_loss = np.sum(-(y_val*np.log(a) + (1-y_val)*np.log(1-a)))\n",
    "        self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))\n",
    "        \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualLayer(SingleLayer):\n",
    "    \n",
    "    def __init__(self, units=10, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.units = units         # 은닉층의 뉴런 개수\n",
    "        self.w1 = None             # 은닉층의 가중치\n",
    "        self.b1 = None             # 은닉층의 절편\n",
    "        self.w2 = None             # 출력층의 가중치\n",
    "        self.b2 = None             # 출력층의 절편\n",
    "        self.a1 = None             # 은닉층의 활성화 출력\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식을 계산합니다\n",
    "        self.a1 = self.activation(z1)            # 활성화 함수를 적용합니다\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식을 계산합니다.\n",
    "        return z2\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)       # 샘플 개수\n",
    "        # 출력층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w2_grad = np.dot(self.a1.T, err) / m\n",
    "        b2_grad = np.sum(err) / m\n",
    "        # 시그모이드 함수까지 그래디언트를 계산합니다.\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        # 은닉층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "\n",
    "    def init_weights(self, n_features):\n",
    "        self.w1 = np.ones((n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)               # 은닉층의 크기\n",
    "        self.w2 = np.ones((self.units, 1))           # (은닉층의 크기, 1)\n",
    "        self.b2 = 0\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)          # 타깃을 열 벡터로 바꿉니다.\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        m = len(x)                    # 샘플 개수를 저장합니다.\n",
    "        self.init_weights(x.shape[1]) # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            a = self.training(x, y, m)\n",
    "            # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / m)\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "            \n",
    "    def training(self, x, y, m):\n",
    "        z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
    "        a = self.activation(z)    # 활성화 함수를 적용합니다.\n",
    "        err = -(y - a)            # 오차를 계산합니다.\n",
    "        # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        # 그래디언트에서 페널티 항의 미분 값을 뺍니다\n",
    "        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
    "        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n",
    "        # 은닉층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w1 -= self.lr * w1_grad\n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        # 출력층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w2 -= self.lr * w2_grad\n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a\n",
    "    \n",
    "    def reg_loss(self):\n",
    "        # 은닉층과 출력층의 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \\\n",
    "               self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomInitNetwork(DualLayer):\n",
    "    \n",
    "    def init_weights(self, n_features):\n",
    "        np.random.seed(42)\n",
    "        self.w1 = np.random.normal(0, 1, \n",
    "                                   (n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)                        # 은닉층의 크기\n",
    "        self.w2 = np.random.normal(0, 1, \n",
    "                                   (self.units, 1))           # (은닉층의 크기, 1)\n",
    "        self.b2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchNetwork(RandomInitNetwork):\n",
    "    \n",
    "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
    "        super().__init__(units, learning_rate, l1, l2)\n",
    "        self.batch_size = batch_size     # 배치 크기\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y_val = y_val.reshape(-1, 1)     # 타깃을 열 벡터로 바꿉니다.\n",
    "        self.init_weights(x.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        np.random.seed(42)\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
    "            for x_batch, y_batch in self.gen_batch(x, y):\n",
    "                y_batch = y_batch.reshape(-1, 1) # 타깃을 열 벡터로 바꿉니다.\n",
    "                m = len(x_batch)                 # 샘플 개수를 저장합니다.\n",
    "                a = self.training(x_batch, y_batch, m)\n",
    "                # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "                loss += np.sum(-(y_batch*np.log(a) + (1-y_batch)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / len(x))\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "\n",
    "    # 미니배치 제너레이터 함수\n",
    "    def gen_batch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size # 미니배치 횟수\n",
    "        if length % self.batch_size:\n",
    "            bins += 1                    # 나누어 떨어지지 않을 때\n",
    "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다.\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i\n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.978021978021978"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_net = MinibatchNetwork(l2=0.01, batch_size=32)\n",
    "minibatch_net.fit(x_train_scaled, y_train, \n",
    "                  x_val=x_val_scaled, y_val=y_val, epochs=500)\n",
    "minibatch_net.score(x_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(minibatch_net.losses)\n",
    "plt.plot(minibatch_net.val_losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
